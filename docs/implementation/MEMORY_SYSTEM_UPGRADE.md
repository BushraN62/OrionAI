# Memory System Upgrade: ChromaDB + SQLite

## üéØ Objectives

1. **Semantic Search**: Find contextually relevant memories, not just keyword matches
2. **Rich Metadata**: Store structured data for filtering and analytics
3. **Efficient Retrieval**: Fast queries with proper indexing
4. **Scalability**: Handle millions of interactions
5. **Context Awareness**: Understand conversation flow and topics

## üèóÔ∏è Architecture Design

### Current System (Simple JSON)
```
data/memory.json
- Linear search
- No semantic understanding
- Limited metadata
- Slow with large datasets
```

### New Hybrid System (ChromaDB + SQLite)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Memory Manager                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   ChromaDB       ‚îÇ         ‚îÇ      SQLite          ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  (Vector Store)  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  (Metadata Store)    ‚îÇ     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Embeddings     ‚îÇ         ‚îÇ ‚Ä¢ Conversations      ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Semantic Search‚îÇ         ‚îÇ ‚Ä¢ Interactions       ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Similarity     ‚îÇ         ‚îÇ ‚Ä¢ Topics             ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Collections    ‚îÇ         ‚îÇ ‚Ä¢ Entities           ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Distance Calc  ‚îÇ         ‚îÇ ‚Ä¢ Analytics          ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ           ‚îÇ                            ‚îÇ                     ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ                        ‚îÇ                                     ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ              ‚îÇ  Embedding Model  ‚îÇ                          ‚îÇ
‚îÇ              ‚îÇ  (sentence-trans  ‚îÇ                          ‚îÇ
‚îÇ              ‚îÇ   formers)        ‚îÇ                          ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìä Database Schema

### SQLite Tables

#### 1. Conversations Table
```sql
CREATE TABLE conversations (
    id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    user_id TEXT,
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ended_at TIMESTAMP,
    message_count INTEGER DEFAULT 0,
    topic TEXT,
    sentiment TEXT, -- positive, neutral, negative
    language TEXT DEFAULT 'en',
    metadata JSON,
    FOREIGN KEY (session_id) REFERENCES sessions(id)
);

CREATE INDEX idx_session_id ON conversations(session_id);
CREATE INDEX idx_started_at ON conversations(started_at);
CREATE INDEX idx_topic ON conversations(topic);
```

#### 2. Interactions Table
```sql
CREATE TABLE interactions (
    id TEXT PRIMARY KEY,
    conversation_id TEXT NOT NULL,
    role TEXT NOT NULL CHECK(role IN ('user', 'assistant')),
    content TEXT NOT NULL,
    embedding_id TEXT, -- Reference to ChromaDB
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    token_count INTEGER,
    model_used TEXT,
    latency_ms INTEGER,
    metadata JSON,
    FOREIGN KEY (conversation_id) REFERENCES conversations(id)
);

CREATE INDEX idx_conversation_id ON interactions(conversation_id);
CREATE INDEX idx_timestamp ON interactions(timestamp);
CREATE INDEX idx_role ON interactions(role);
CREATE INDEX idx_embedding_id ON interactions(embedding_id);
```

#### 3. Topics Table
```sql
CREATE TABLE topics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT UNIQUE NOT NULL,
    category TEXT,
    frequency INTEGER DEFAULT 1,
    first_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_name ON topics(name);
CREATE INDEX idx_category ON topics(category);
```

#### 4. Entities Table (Named Entity Recognition)
```sql
CREATE TABLE entities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    interaction_id TEXT NOT NULL,
    entity_type TEXT NOT NULL, -- PERSON, ORG, LOCATION, DATE, etc.
    entity_value TEXT NOT NULL,
    confidence REAL,
    context TEXT,
    FOREIGN KEY (interaction_id) REFERENCES interactions(id)
);

CREATE INDEX idx_interaction_id ON entities(interaction_id);
CREATE INDEX idx_entity_type ON entities(entity_type);
CREATE INDEX idx_entity_value ON entities(entity_value);
```

#### 5. Memory Links (Relationships)
```sql
CREATE TABLE memory_links (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_interaction_id TEXT NOT NULL,
    target_interaction_id TEXT NOT NULL,
    link_type TEXT, -- reference, followup, contradiction, agreement
    strength REAL DEFAULT 0.5, -- 0.0 to 1.0
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (source_interaction_id) REFERENCES interactions(id),
    FOREIGN KEY (target_interaction_id) REFERENCES interactions(id)
);

CREATE INDEX idx_source_interaction ON memory_links(source_interaction_id);
CREATE INDEX idx_target_interaction ON memory_links(target_interaction_id);
```

#### 6. User Preferences
```sql
CREATE TABLE user_preferences (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id TEXT NOT NULL,
    preference_key TEXT NOT NULL,
    preference_value TEXT NOT NULL,
    inferred_from TEXT, -- interaction_id that revealed this preference
    confidence REAL DEFAULT 0.5,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(user_id, preference_key)
);

CREATE INDEX idx_user_id ON user_preferences(user_id);
```

#### 7. Analytics Table
```sql
CREATE TABLE analytics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    metric_name TEXT NOT NULL,
    metric_value REAL NOT NULL,
    dimension TEXT, -- daily, weekly, monthly
    recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSON
);

CREATE INDEX idx_metric_name ON analytics(metric_name);
CREATE INDEX idx_recorded_at ON analytics(recorded_at);
```

### ChromaDB Collections

#### 1. Interactions Collection
```python
{
    "collection_name": "orion_interactions",
    "embedding_function": "sentence-transformers/all-MiniLM-L6-v2",
    "metadata": {
        "id": "uuid",
        "conversation_id": "uuid",
        "role": "user|assistant",
        "timestamp": "iso8601",
        "topic": "string",
        "sentiment": "positive|neutral|negative",
        "language": "en",
        "tokens": 150,
        "session_id": "uuid"
    },
    "documents": ["The actual text content..."]
}
```

#### 2. Long-term Knowledge Collection
```python
{
    "collection_name": "orion_knowledge",
    "embedding_function": "sentence-transformers/all-MiniLM-L6-v2",
    "metadata": {
        "id": "uuid",
        "fact_type": "user_preference|learned_fact|instruction",
        "confidence": 0.95,
        "first_learned": "iso8601",
        "last_validated": "iso8601",
        "source_interaction_ids": ["uuid1", "uuid2"],
        "importance": 0.8
    },
    "documents": ["User prefers Python over JavaScript"]
}
```

## üîß Implementation

### 1. Enhanced Memory Manager

```python
# orion/app/memory/enhanced_memory.py

import sqlite3
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import json
import uuid
from pathlib import Path

class EnhancedMemoryManager:
    def __init__(self, db_path: str = "data/memory.db", chroma_path: str = "data/chroma"):
        """Initialize hybrid memory system"""
        
        # SQLite connection
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self._initialize_database()
        
        # ChromaDB client
        Path(chroma_path).mkdir(parents=True, exist_ok=True)
        self.chroma_client = chromadb.PersistentClient(
            path=chroma_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Collections
        self.interactions_collection = self.chroma_client.get_or_create_collection(
            name="orion_interactions",
            metadata={"description": "All user-assistant interactions"}
        )
        
        self.knowledge_collection = self.chroma_client.get_or_create_collection(
            name="orion_knowledge",
            metadata={"description": "Extracted long-term knowledge"}
        )
        
        # Embedding model
        self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    
    def _initialize_database(self):
        """Create all SQLite tables"""
        cursor = self.conn.cursor()
        
        # Conversations table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS conversations (
                id TEXT PRIMARY KEY,
                session_id TEXT NOT NULL,
                user_id TEXT,
                started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                ended_at TIMESTAMP,
                message_count INTEGER DEFAULT 0,
                topic TEXT,
                sentiment TEXT,
                language TEXT DEFAULT 'en',
                metadata JSON
            )
        """)
        
        # Interactions table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS interactions (
                id TEXT PRIMARY KEY,
                conversation_id TEXT NOT NULL,
                role TEXT NOT NULL CHECK(role IN ('user', 'assistant')),
                content TEXT NOT NULL,
                embedding_id TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                token_count INTEGER,
                model_used TEXT,
                latency_ms INTEGER,
                metadata JSON,
                FOREIGN KEY (conversation_id) REFERENCES conversations(id)
            )
        """)
        
        # Topics table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS topics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE NOT NULL,
                category TEXT,
                frequency INTEGER DEFAULT 1,
                first_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Entities table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS entities (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                interaction_id TEXT NOT NULL,
                entity_type TEXT NOT NULL,
                entity_value TEXT NOT NULL,
                confidence REAL,
                context TEXT,
                FOREIGN KEY (interaction_id) REFERENCES interactions(id)
            )
        """)
        
        # Memory links table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS memory_links (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_interaction_id TEXT NOT NULL,
                target_interaction_id TEXT NOT NULL,
                link_type TEXT,
                strength REAL DEFAULT 0.5,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_interaction_id) REFERENCES interactions(id),
                FOREIGN KEY (target_interaction_id) REFERENCES interactions(id)
            )
        """)
        
        # User preferences table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS user_preferences (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                preference_key TEXT NOT NULL,
                preference_value TEXT NOT NULL,
                inferred_from TEXT,
                confidence REAL DEFAULT 0.5,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(user_id, preference_key)
            )
        """)
        
        # Analytics table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS analytics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                metric_name TEXT NOT NULL,
                metric_value REAL NOT NULL,
                dimension TEXT,
                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                metadata JSON
            )
        """)
        
        # Create indexes
        cursor.executescript("""
            CREATE INDEX IF NOT EXISTS idx_session_id ON conversations(session_id);
            CREATE INDEX IF NOT EXISTS idx_conversation_id ON interactions(conversation_id);
            CREATE INDEX IF NOT EXISTS idx_timestamp ON interactions(timestamp);
            CREATE INDEX IF NOT EXISTS idx_topic_name ON topics(name);
            CREATE INDEX IF NOT EXISTS idx_entity_type ON entities(entity_type);
            CREATE INDEX IF NOT EXISTS idx_user_prefs ON user_preferences(user_id);
        """)
        
        self.conn.commit()
    
    def add_interaction(
        self,
        conversation_id: str,
        role: str,
        content: str,
        session_id: str,
        model_used: Optional[str] = None,
        latency_ms: Optional[int] = None,
        metadata: Optional[Dict] = None
    ) -> str:
        """Add interaction to both SQLite and ChromaDB"""
        
        interaction_id = str(uuid.uuid4())
        timestamp = datetime.utcnow().isoformat()
        
        # Generate embedding
        embedding = self.embedding_model.encode(content).tolist()
        
        # Store in ChromaDB
        self.interactions_collection.add(
            ids=[interaction_id],
            embeddings=[embedding],
            documents=[content],
            metadatas=[{
                "conversation_id": conversation_id,
                "role": role,
                "timestamp": timestamp,
                "session_id": session_id,
                "model_used": model_used or "unknown"
            }]
        )
        
        # Store in SQLite
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO interactions 
            (id, conversation_id, role, content, embedding_id, timestamp, 
             model_used, latency_ms, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            interaction_id,
            conversation_id,
            role,
            content,
            interaction_id,  # Same as interaction_id for ChromaDB reference
            timestamp,
            model_used,
            latency_ms,
            json.dumps(metadata) if metadata else None
        ))
        
        self.conn.commit()
        return interaction_id
    
    def semantic_search(
        self,
        query: str,
        n_results: int = 5,
        filter_metadata: Optional[Dict] = None
    ) -> List[Dict]:
        """
        Search for semantically similar interactions
        
        Args:
            query: Search query
            n_results: Number of results to return
            filter_metadata: Optional metadata filters (e.g., {"role": "user"})
        
        Returns:
            List of relevant interactions with similarity scores
        """
        
        # Generate query embedding
        query_embedding = self.embedding_model.encode(query).tolist()
        
        # Search ChromaDB
        results = self.interactions_collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=filter_metadata
        )
        
        # Enrich with SQLite data
        enriched_results = []
        for i, doc_id in enumerate(results['ids'][0]):
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT * FROM interactions WHERE id = ?
            """, (doc_id,))
            
            row = cursor.fetchone()
            if row:
                enriched_results.append({
                    "id": row['id'],
                    "content": row['content'],
                    "role": row['role'],
                    "timestamp": row['timestamp'],
                    "similarity_score": 1 - results['distances'][0][i],  # Convert distance to similarity
                    "metadata": json.loads(row['metadata']) if row['metadata'] else {}
                })
        
        return enriched_results
    
    def get_conversation_context(
        self,
        conversation_id: str,
        max_interactions: int = 10
    ) -> List[Dict]:
        """Get recent interactions from a conversation"""
        
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT id, role, content, timestamp, model_used, latency_ms
            FROM interactions
            WHERE conversation_id = ?
            ORDER BY timestamp DESC
            LIMIT ?
        """, (conversation_id, max_interactions))
        
        interactions = []
        for row in cursor.fetchall():
            interactions.append({
                "id": row['id'],
                "role": row['role'],
                "content": row['content'],
                "timestamp": row['timestamp'],
                "model_used": row['model_used'],
                "latency_ms": row['latency_ms']
            })
        
        return list(reversed(interactions))  # Chronological order
    
    def extract_and_store_knowledge(
        self,
        interaction_id: str,
        knowledge_text: str,
        fact_type: str = "learned_fact",
        confidence: float = 0.8
    ):
        """Extract important knowledge and store in long-term memory"""
        
        knowledge_id = str(uuid.uuid4())
        timestamp = datetime.utcnow().isoformat()
        
        # Generate embedding
        embedding = self.embedding_model.encode(knowledge_text).tolist()
        
        # Store in ChromaDB knowledge collection
        self.knowledge_collection.add(
            ids=[knowledge_id],
            embeddings=[embedding],
            documents=[knowledge_text],
            metadatas=[{
                "fact_type": fact_type,
                "confidence": confidence,
                "first_learned": timestamp,
                "last_validated": timestamp,
                "source_interaction_id": interaction_id,
                "importance": confidence
            }]
        )
        
        return knowledge_id
    
    def find_related_memories(
        self,
        current_interaction: str,
        n_results: int = 5
    ) -> List[Dict]:
        """Find related past interactions using semantic similarity"""
        
        return self.semantic_search(current_interaction, n_results)
    
    def get_user_preferences(self, user_id: str) -> Dict[str, any]:
        """Get all known user preferences"""
        
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT preference_key, preference_value, confidence, updated_at
            FROM user_preferences
            WHERE user_id = ?
            ORDER BY confidence DESC
        """, (user_id,))
        
        preferences = {}
        for row in cursor.fetchall():
            preferences[row['preference_key']] = {
                "value": row['preference_value'],
                "confidence": row['confidence'],
                "updated_at": row['updated_at']
            }
        
        return preferences
    
    def update_user_preference(
        self,
        user_id: str,
        key: str,
        value: str,
        inferred_from: Optional[str] = None,
        confidence: float = 0.8
    ):
        """Update or create user preference"""
        
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO user_preferences 
            (user_id, preference_key, preference_value, inferred_from, confidence, updated_at)
            VALUES (?, ?, ?, ?, ?, ?)
            ON CONFLICT(user_id, preference_key) DO UPDATE SET
                preference_value = excluded.preference_value,
                confidence = excluded.confidence,
                updated_at = excluded.updated_at
        """, (user_id, key, value, inferred_from, confidence, datetime.utcnow().isoformat()))
        
        self.conn.commit()
    
    def get_topic_trends(self, limit: int = 10) -> List[Dict]:
        """Get most discussed topics"""
        
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT name, category, frequency, last_seen
            FROM topics
            ORDER BY frequency DESC
            LIMIT ?
        """, (limit,))
        
        return [dict(row) for row in cursor.fetchall()]
    
    def close(self):
        """Close connections"""
        self.conn.close()
```

### 2. Integration with Orion Brain

```python
# orion_brain.py (additions)

from orion.app.memory.enhanced_memory import EnhancedMemoryManager

class OrionBrain:
    def __init__(self):
        # ... existing code ...
        
        # Enhanced memory system
        self.memory = EnhancedMemoryManager()
        self.current_conversation_id = None
    
    def start_conversation(self, session_id: str) -> str:
        """Start new conversation"""
        conversation_id = str(uuid.uuid4())
        
        cursor = self.memory.conn.cursor()
        cursor.execute("""
            INSERT INTO conversations (id, session_id, started_at)
            VALUES (?, ?, ?)
        """, (conversation_id, session_id, datetime.utcnow().isoformat()))
        
        self.memory.conn.commit()
        self.current_conversation_id = conversation_id
        
        return conversation_id
    
    async def process_with_memory(
        self,
        user_message: str,
        session_id: str,
        use_semantic_context: bool = True
    ):
        """Process message with enhanced memory"""
        
        if not self.current_conversation_id:
            self.current_conversation_id = self.start_conversation(session_id)
        
        # Store user message
        user_interaction_id = self.memory.add_interaction(
            conversation_id=self.current_conversation_id,
            role="user",
            content=user_message,
            session_id=session_id
        )
        
        # Find relevant context
        context_memories = []
        if use_semantic_context:
            context_memories = self.memory.semantic_search(
                query=user_message,
                n_results=5,
                filter_metadata={"role": "assistant"}  # Get past assistant responses
            )
        
        # Build context-aware prompt
        context_text = "\n".join([
            f"[Relevant memory {i+1}]: {mem['content'][:200]}..."
            for i, mem in enumerate(context_memories)
        ])
        
        enhanced_prompt = f"""
Context from past conversations:
{context_text}

Current user message: {user_message}

Respond considering the context above.
"""
        
        # Generate response
        response = await self.generate_response(enhanced_prompt)
        
        # Store assistant response
        assistant_interaction_id = self.memory.add_interaction(
            conversation_id=self.current_conversation_id,
            role="assistant",
            content=response,
            session_id=session_id,
            model_used=self.current_model
        )
        
        return response
```

## üìà Performance Optimizations

### 1. Embedding Caching
```python
class EmbeddingCache:
    def __init__(self, cache_size: int = 1000):
        self.cache = {}
        self.cache_size = cache_size
    
    def get_or_create(self, text: str, embedding_fn) -> List[float]:
        text_hash = hashlib.md5(text.encode()).hexdigest()
        
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        embedding = embedding_fn(text)
        
        if len(self.cache) >= self.cache_size:
            # Remove oldest entry
            self.cache.pop(next(iter(self.cache)))
        
        self.cache[text_hash] = embedding
        return embedding
```

### 2. Batch Processing
```python
def add_interactions_batch(self, interactions: List[Dict]):
    """Add multiple interactions efficiently"""
    
    texts = [i['content'] for i in interactions]
    embeddings = self.embedding_model.encode(texts, batch_size=32)
    
    # Batch insert to ChromaDB
    self.interactions_collection.add(
        ids=[i['id'] for i in interactions],
        embeddings=embeddings.tolist(),
        documents=texts,
        metadatas=[i['metadata'] for i in interactions]
    )
    
    # Batch insert to SQLite
    cursor = self.conn.cursor()
    cursor.executemany("""
        INSERT INTO interactions (...) VALUES (?, ?, ?, ...)
    """, [(i['id'], i['conversation_id'], ...) for i in interactions])
    
    self.conn.commit()
```

### 3. Smart Context Window
```python
def get_adaptive_context(
    self,
    query: str,
    max_tokens: int = 2000,
    relevance_threshold: float = 0.7
) -> List[Dict]:
    """Get context that fits within token budget and relevance threshold"""
    
    # Start with semantic search
    candidates = self.semantic_search(query, n_results=20)
    
    # Filter by relevance
    relevant = [c for c in candidates if c['similarity_score'] >= relevance_threshold]
    
    # Fit within token budget
    selected = []
    token_count = 0
    
    for memory in relevant:
        memory_tokens = len(memory['content'].split()) * 1.3  # Rough estimate
        if token_count + memory_tokens <= max_tokens:
            selected.append(memory)
            token_count += memory_tokens
        else:
            break
    
    return selected
```

## üîç Advanced Features

### 1. Topic Modeling
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

def extract_topics(self, n_topics: int = 10):
    """Extract topics from conversations"""
    
    cursor = self.conn.cursor()
    cursor.execute("SELECT content FROM interactions WHERE role = 'user'")
    
    documents = [row['content'] for row in cursor.fetchall()]
    
    # TF-IDF
    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
    doc_term_matrix = vectorizer.fit_transform(documents)
    
    # LDA
    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
    lda.fit(doc_term_matrix)
    
    # Extract top words per topic
    feature_names = vectorizer.get_feature_names_out()
    topics = []
    
    for idx, topic in enumerate(lda.components_):
        top_words = [feature_names[i] for i in topic.argsort()[-10:]]
        topics.append({
            "id": idx,
            "words": top_words,
            "weight": topic.sum()
        })
    
    return topics
```

### 2. Memory Consolidation (Sleep Mode)
```python
async def consolidate_memories(self):
    """Run periodically to consolidate important memories"""
    
    # Find frequently accessed interactions
    cursor = self.conn.cursor()
    cursor.execute("""
        SELECT i.id, i.content, COUNT(ml.id) as reference_count
        FROM interactions i
        LEFT JOIN memory_links ml ON i.id = ml.target_interaction_id
        GROUP BY i.id
        HAVING reference_count > 3
    """)
    
    important_interactions = cursor.fetchall()
    
    for interaction in important_interactions:
        # Extract knowledge
        knowledge = await self.llm_extract_knowledge(interaction['content'])
        
        if knowledge:
            self.extract_and_store_knowledge(
                interaction_id=interaction['id'],
                knowledge_text=knowledge,
                fact_type="consolidated",
                confidence=0.9
            )
```

### 3. Forgetting Mechanism
```python
def apply_forgetting_curve(self, days_old: int = 30):
    """Remove or downgrade old, unused memories"""
    
    cutoff_date = (datetime.utcnow() - timedelta(days=days_old)).isoformat()
    
    # Find unused old interactions
    cursor = self.conn.cursor()
    cursor.execute("""
        SELECT id FROM interactions
        WHERE timestamp < ?
        AND id NOT IN (
            SELECT DISTINCT source_interaction_id FROM memory_links
            UNION
            SELECT DISTINCT target_interaction_id FROM memory_links
        )
    """, (cutoff_date,))
    
    old_ids = [row['id'] for row in cursor.fetchall()]
    
    # Archive or delete
    for interaction_id in old_ids:
        self.interactions_collection.delete(ids=[interaction_id])
        cursor.execute("DELETE FROM interactions WHERE id = ?", (interaction_id,))
    
    self.conn.commit()
    
    return len(old_ids)
```

## üìä Analytics & Insights

### Query Examples

```python
# 1. Conversation statistics
def get_conversation_stats(self, session_id: str) -> Dict:
    cursor = self.conn.cursor()
    
    stats = cursor.execute("""
        SELECT 
            COUNT(DISTINCT c.id) as total_conversations,
            SUM(c.message_count) as total_messages,
            AVG(i.latency_ms) as avg_latency,
            COUNT(DISTINCT t.name) as unique_topics
        FROM conversations c
        LEFT JOIN interactions i ON c.id = i.conversation_id
        LEFT JOIN topics t ON c.topic = t.name
        WHERE c.session_id = ?
    """, (session_id,)).fetchone()
    
    return dict(stats)

# 2. User interaction patterns
def get_user_patterns(self, user_id: str) -> Dict:
    cursor = self.conn.cursor()
    
    # Peak activity hours
    peak_hours = cursor.execute("""
        SELECT strftime('%H', timestamp) as hour, COUNT(*) as count
        FROM interactions
        WHERE conversation_id IN (
            SELECT id FROM conversations WHERE user_id = ?
        )
        GROUP BY hour
        ORDER BY count DESC
        LIMIT 3
    """, (user_id,)).fetchall()
    
    # Most discussed topics
    top_topics = cursor.execute("""
        SELECT t.name, t.frequency
        FROM topics t
        JOIN conversations c ON t.name = c.topic
        WHERE c.user_id = ?
        ORDER BY t.frequency DESC
        LIMIT 5
    """, (user_id,)).fetchall()
    
    return {
        "peak_hours": [dict(row) for row in peak_hours],
        "top_topics": [dict(row) for row in top_topics]
    }

# 3. Memory graph visualization
def get_memory_graph(self, interaction_id: str, depth: int = 2):
    """Get connected memories for visualization"""
    
    cursor = self.conn.cursor()
    
    # Recursive CTE to find connected memories
    cursor.execute("""
        WITH RECURSIVE memory_graph AS (
            SELECT source_interaction_id, target_interaction_id, link_type, strength, 0 as depth
            FROM memory_links
            WHERE source_interaction_id = ?
            
            UNION ALL
            
            SELECT ml.source_interaction_id, ml.target_interaction_id, ml.link_type, ml.strength, mg.depth + 1
            FROM memory_links ml
            JOIN memory_graph mg ON ml.source_interaction_id = mg.target_interaction_id
            WHERE mg.depth < ?
        )
        SELECT DISTINCT * FROM memory_graph
    """, (interaction_id, depth))
    
    return [dict(row) for row in cursor.fetchall()]
```

## üöÄ Migration Plan

### Phase 1: Parallel Running (Week 1-2)
- Keep existing JSON memory
- Add hybrid system alongside
- Dual-write to both systems
- Compare results

### Phase 2: Testing (Week 3-4)
- Run A/B tests
- Measure performance improvements
- Validate data consistency
- Tune parameters

### Phase 3: Migration (Week 5)
- Import historical data
- Switch primary system
- Deprecate JSON memory
- Monitor for issues

### Phase 4: Optimization (Week 6+)
- Fine-tune embeddings
- Optimize queries
- Add advanced features
- Scale testing

## üì¶ Dependencies

```txt
# requirements.txt additions

# Vector Database
chromadb>=0.4.18
sentence-transformers>=2.2.2

# Database
sqlite3  # Built-in Python

# NLP & ML
spacy>=3.7.0
sklearn>=1.3.0

# Performance
orjson>=3.9.0  # Faster JSON
```

## üéØ Expected Improvements

| Metric | Current | With Hybrid System | Improvement |
|--------|---------|-------------------|-------------|
| Context Relevance | 60% | 85%+ | +41% |
| Query Speed | 500ms | 50ms | 10x faster |
| Storage Efficiency | Linear | Indexed | 100x better |
| Scalability | 10K interactions | 10M+ interactions | 1000x |
| Semantic Understanding | None | High | ‚àû |

## üí° Use Cases

1. **"Remember when we talked about..."** ‚Üí Semantic search finds it
2. **Personalized responses** ‚Üí User preferences from SQLite
3. **Topic continuity** ‚Üí Related memory links
4. **Learning user patterns** ‚Üí Analytics queries
5. **Smart context** ‚Üí Only relevant memories in prompt
6. **Knowledge extraction** ‚Üí Long-term knowledge base
7. **Conversation insights** ‚Üí Topic trends, sentiment analysis

## üîê Privacy Considerations

- All data stored locally (ChromaDB + SQLite)
- No cloud embeddings (local sentence-transformers)
- Encrypted database option (SQLCipher)
- Privacy mode: disable memory entirely
- Data export/deletion tools
- Retention policies

---

**Next Steps**: Would you like me to implement this system? I can start with:
1. Creating the `EnhancedMemoryManager` class
2. Setting up the database schema
3. Adding API endpoints for memory operations
4. Building a memory visualization dashboard
